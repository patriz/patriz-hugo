+++
date          = "2018-12-05T12:58:00+00:00"
draft         = true
title         = "[번역] 확장 가능한 포터블 도커 컨테이너 네트워크 디자인"
tags          = ["Networking", "Docker"]
categories    = ["DevOps"]
slug          = "designing-scalable-portable-docker-container-networks"
notoc         = false
socialsharing = true
nocomment     = false
+++


# 배우게 될 것

도커 컨테이너는 실행에 필요한 모든 것, 즉 코드, 런타임, 시스템 도구, 시스템 라이브러리 등 서버에 설치할 수 있는 모든 것을 포함하는 완전한 파일 시스템에 있는 소프트웨어 하나를 래핑한다. 이는 소프트웨어가 환경에 관계없이 항상 동일한 기능을 실행할 것을 보장한다. 기본적으로 컨테이너는 애플리케이션을 서로 격리하고, 애플리케이션을 위한 추가적인 보호 계층을 제공한다.

애플리케이션이 서로 간에, 또는 호스트와, 혹은 외부 네트워크와 통신해야 한다면 어떻게 해야하는가? 애플리케이션 이식성, 서비스 검색, 로드 밸런싱, 보안, 성능 및 확장성을 유지하면서 적절한 연결성을 위해 네트워크를 설계하는 방법은 무엇인가? 이 문서는 이러한 네트워크 설계 문제와 사용 가능한 도구 및 일반적인 배포 패턴을 다룬다. 물리적 네트워크 설계를 지정하거나 권장하지는 않지만 애플리케이션과 물리적 네트워크의 제약조건을 고려하면서 도커 네트워크를 설계하는 방법에 대한 옵션들을 제공한다.

## 사전 지식
계속하기 전에 도커 개념과 도커 스웜에 대해 잘 알고 있어야 한다.

- [도커 개념](https://docs.docker.com/engine/understanding-docker/)
- [도커 스웜 및 스웜 모드 개념](https://docs.docker.com/engine/swarm/key-concepts/#/services-and-tasks)

# 네트워킹 컨테이너 및 마이크로 서비스의 도전

마이크로서비스는 실전에서 애플리케이션의 스케일을 증가시켰는데, 이러한 애플리케이션들은 제공되는 연결성 및 격리 방법들이 훨씬 더 중시된다. 도커 네트워킹 철학은 애플리케이션 중심이다. 네트워크 운영자에게 옵션과 유연성을 제공하는 것을 목표로 하는 것 뿐만 아니라 애플리케이션 개발자에게 적합한 수준의 추상화를 제공하는 것을 목표로 한다.

여느 디자인처럼, 네트워크 디자인은 밸런스을 맞추는 행위이다. 도커 엔터프라이즈 에디션과 도커 생태계는 네트워크 엔지니어들에게 그들의 애플리케이션과 환경에 대한 최상의 밸런스를 달성하기 위해 다양한 도구를 제공한다. 각 옵션은 서로 다른 장점과 트레이드오프를 제공한다. 이 가이드의 나머지 부분에서는 네트워크 엔지니어가 자신의 환경에 가장 적합한 것을 이해할 수 있도록 이러한 각각 선택에 대해 자세히 설명한다.

도커는 애플리케이션을 공급하는 새로운 방법을 개발했고, 그와 함께 컨테이너는 네트워킹 접근 방식의 일부 측면도 변화시켰다. 다음 주제는 컨테이너형 애플리케이션에 대한 일반적인 디자인 주제이다.

- **이식성**
	- 고유한 네트워크 특성을 활용하면서 다양한 네트워크 환경에서 최대한의 이식성을 어떻게 보장할 수 있는가?

- **서비스 검색**
	- 서비스가 확장 및 축소될 때 어디에 살고 있는지 어떻게 알 수 있는가?

- **로드 밸런싱**
	- 서비스 자체가 커지고 확장될 때 서비스 간에 부하를 어떻게 공유하는가?

- **보안**
	- 잘못된 컨테이너가 서로 액세스하지 못하도록 어떻게 구분하는가?
	- 애플리케이션 및 클러스터 제어 트래픽이 있는 컨테이너가 안전하다는 것을 어떻게 보장하는가?

- **성능**
	- 대기 시간을 최소화하고 대역폭을 최대화하는 동시에 고급 네트워크 서비스를 어떻게 제공하는가?

- **확장성**
	- 여러 호스트에 걸쳐 애플리케이션을 확장할 때 이러한 특성들이 저하되지 않도록 하려면 어떻게 하는가?


# 컨테이너 네트워킹 모델

도커 네트워킹 아키텍처는 CNM(Container Networking Model)이라고 하는 인터페이스 집합 위에 구축된다. CNM의 철학은 다양한 인프라 구조에서 애플리케이션 이식성을 제공하는 것이다. 이 모델은 애플리케이션 이식성을 달성하기 위해 밸런스를 맞추고 인프라의 특수 기능과 역량을 활용한다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fcnm.png)

## CNM 구조

CNM에는 몇 가지 상위 레벨의 구조가 있다. 이들은 모두 OS 및 인프라에 구애받지 않으므로 인프라 스택에 관계없이 애플리케이션이 동일한 환경을 가질 수 있다.

- **샌드박스** - 샌드박스는 컨테이너 네트워크 스택의 구성을 포함한다. 여기에는 컨테이너 인터페이스, 라우팅 테이블 및 DNS 설정 관리가 포함된다. 샌드박스의 구현은 Linux 네트워크 네임스페이스, FreeBSD Jail 또는 기타 유사한 개념일 수 있다. 샌드박스는 여러 네트워크의 많은 엔드포인트를 포함할 수 있다.
- **엔드포인트** - 엔드포인트가 샌드박스를 네트워크에 연결한다. 엔드포인트의 구조는 네트워크에 대한 실제 연결을 애플리케이션에서 추상화할 수 있도록 존재한다. 이는 서비스가 네트워크에 어떻게 연결되어 있는지에 신경쓰지 않으며 여러 유형의 네트워크 드라이버를 사용할 수 있도록 이식성을 유지하는데 도움을 준다.
- **네트워크** — CNM은 OSI 모델 측면에서 네트워크를 지정하지 않는다. 네트워크의 구현은 리눅스 브리지, VLAN 등이 될 수 있다. 네트워크는 이들 사이에 연결성이 있는 엔드포인트의 모음이다. 네트워크에 연결되지 않은 엔드포인트는 네트워크에 연결되어 있지 않다.

## CNM 드라이버 인터페이스
컨테이너 네트워킹 모델은 사용자, 커뮤니티 및 벤더가 네트워크에서 추가 기능, 가시성 또는 제어 기능을 활용할 수 있는 2개의 플러그형 및 개방형 인터페이스를 제공한다.

다음 네트워크 드라이버가 있다:

- **네트워크 드라이버** - 도커 네트워크 드라이버는 네트워크를 작동시키는 실제 구현을 제공한다. 다른 드라이버를 사용할 수 있고 다양한 유스케이스를 쉽게 지원할 수 있도록 상호 교환될 수 있는 플러그 스타일이다. 주어진 도커 엔진 또는 클러스터에서 여러 개의 네트워크 드라이버를 동시에 사용할 수 있지만, 각 도커 네트워크는 단일 네트워크 드라이버를 통해서만 인스턴스화된다. CNM 네트워크 드라이버는 크게 두 가지 유형이 있다.
	- **네이티브 네트워크 드라이버** - 네이티브 네트워크 드라이버는 도커 엔진의 기본 요소이며 도커에서 제공한다. 오버레이 네트워크나 로컬 브리지와 같은 다양한 기능을 지원하는 여러 드라이버가 있다.
	- **원격 네트워크 드라이버** - 원격 네트워크 드라이버는 커뮤니티와 다른 벤더가 생성한 네트워크 드라이버이다. 이러한 드라이버는 기존 소프트웨어 및 하드웨어와의 통합을 제공할 수 있다. 사용자는 또한 기존 네트워크 드라이버가 지원하지 않는 특정 기능을 원하는 경우 자신의 드라이버를 만들 수도 있다.
- **IPAM 드라이버** - 도커에는 네트워크 및 엔드포인트의 기본 서브넷 또는 IP 주소를 지정하지 않은 경우 이를 제공하는 네이티브 IP 주소 관리 드라이버가 있다. IP 어드레싱은 네트워크, 컨테이너 및 서비스 생성 명령을 통해 수동으로 할당할 수도 있다. 원격 IPAM 드라이버도 존재하며 기존 IPAM 도구와 통합된다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fcnm-api.png)


## 도커 네이티브 네트워크 드라이버

도커 네이티브 네트워크 드라이버는 도커 엔진의 일부이므로 추가 모듈은 필요없다. 이들은 표준 `docker network` 명령을 통해 호출되고 사용된다. 다음과 같은 네이티브 네트워크 드라이버가 있다.

|  드라이버  |  설명  |
| -------- | ---- |
| Host     | `host` 드라이버가 있는 호스트 컨테이너는 호스트의 네트워킹 스택을 사용함. 네임스페이스 분리가 없으므로 호스트의 모든 인터페이스를 컨테이너에서 직접 사용할 수 있음.|
| Brdige   | `bridge` 드라이버는 도커가 관리하는 호스트에 리눅스 브리지를 생성함. 기본적으로 브리지의 컨테이너는 서로 통신할 수 있음. 또한 브리지 드라이버를 통해 컨테이너에 대한 외부 액세스를 구성할 수 있음. |
| Overlay  | `overlay` 드라이버는 도커 외부에서 다중 호스트 네트워크를 지원하는 오버레이 네트워크를 생성함. 로컬 리눅스 브리지와 VXLAN을 함께 사용하여 물리적 네트워크 인프라를 통해 컨테이너와 컨테이너 간의 통신을 오버레이함. |
| MACVLAN  | `macvlan` 드라이버는 MACVLAN 브리지 모드를 사용해서 컨테이너 인터페이스와 상위 호스트 인터페이스(또는 하위 인터페이스) 간의 연결을 설정함. 물리적 네트워크에서 라우팅 가능한 컨테이너에 IP 주소를 제공하기 위해 사용될 수 있음. 또한 VLAN은 Layer 2 컨테이너 분할을 실행하기 위해 macvlan 드라이버에 트렁킹될 수 있음. |
| None     | `none` 드라이버는 컨테이너에 자체 네트워킹 스택과 네트워크 네임스페이스를 제공하지만, 컨테이너 내부에 인터페이스를 구성하지는 않음. 추가 구성없이 컨테이너가 호스트 네트워킹 스택에서 완전히 분리됨. |

## 네트워크 스코프

`docker network ls` 출력에서 볼 수 있듯이, 도커 네트워크 드라이버는 *스코프* 라는 개념이 있다. 네트워크 스코프는 `local` 또는 `swarm` 범위인 드라이버의 도메인이다. Local 스코프 드라이버는 호스트의 범위 내에서 연결 및 네트워크 서비스(예: DNS 또는 IPAM)를 제공한다. Swarm 스코프 드라이버는 Swarm 클러스터 전반에서 연결 및 네트워크 서비스를 제공한다. Swarm 스코프 네트워크는 전체 클러스터에서 동일한 네트워크 ID를 가지고 있고, 반면에 Local 스코프 네트워크는 각 호스트에 고유한 네트워크 ID를 가지고 있다.

```sh
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
1475f03fbecb        bridge              bridge              local
e2d8a4bd86cb        docker_gwbridge     bridge              local
407c477060e7        host                host                local
f4zr3zrswlyg        ingress             overlay             swarm
c97909a4b198        none                null                local
```

## 도커 원격 네트워크 드라이버
아래 커뮤니티 및 벤더에서 만든 원격 네트워크 드라이버는 CNM과 호환되며, 각 드라이버는 컨테이너를 위한 고유한 기능과 네트워크 서비스를 제공한다.

|드라이버 | 설명 |
|----------------------------------|----------|
|[contiv](http://contiv.github.io/)<img width=250/>| 멀티 테넌트 (Multi-tenant) 마이크로 서비스 구현를 위한 인프라 및 보안 정책을 제공하는 오픈 소스 네트워크 플러그인, Cisco Systems가 리딩. 또한 Contiv는 비컨테이너 워크로드 및 ACI와 같은 물리적 네트워크에 대해 통합됨. Contiv는 원격 네트워크 및 IPAM 드라이버를 구현하고 있음.|
|[weave](https://www.weave.works/docs/net/latest/introducing-weave/)|도커 컨테이너를 여러 호스트 또는 클라우드에 연결하는 가상 네트워크를 만드는 네트워크 플러그인. Weave는 애플리케이션 자동 검색 기능을 제공하며 부분적으로 연결된 네트워크에서 작동할 수 있으며, 외부 클러스터 저장소가 필요하지 않고, 운영 친화적임.|
|[calico](https://www.projectcalico.org/)| 클라우드 데이터 센터의 가상 네트워킹을 위한 오픈 소스 솔루션. 대부분의 워크로드(VM, 컨테이너 또는 베어 메탈 서버)가 IP 연결만 필요로 하는 데이터 센터를 대상으로 함. Calico는 표준 IP 라우팅을 사용하여 이 연결성을 제공함. 테넌트 소유권 또는보다 세분화된 정책에 따라 워크로드 간의 격리는 소스 및 대상 워크로드를 호스팅하는 서버에서 iptables 프로그래밍을 통해 수행됨.|
|[kuryr](https://github.com/openstack/kuryr)|OpenStack Kuryr 프로젝트의 일부로 개발된 네트워크 플러그인. OpenStack 네트워킹 서비스인 Neutron을 활용하여 Docker 네트워킹 (libnetwork) 원격 드라이버 API를 구현함. Kuryr에는 IPAM 드라이버도 포함되어 있음.|

## 도커 원격 IPAM 드라이버

커뮤니티 및 벤더가 생성한 IPAM 드라이버도 기존 시스템 또는 특수 기능과의 통합을 제공하는데 사용될 수 있다.

| 드라이버 | 설명 |
|--------|------|
|[infobox](https://hub.docker.com/r/infoblox/ipam-driver/)| 기존 Infoblox 툴과 통합되는 오픈소스 IPAM 플러그인. |

> 많은 도커 플러그인이 존재하며, 더 많은 플러그인이 계속 만들어지고 있다. 도커는 가장 일반적인 플러그인 목록을 유지한다.

# 리눅스 네트워크 원리

리눅스 커널은 TCP/IP 스택 (DNS 및 VXLAN과 같은 기타 네이티브 커널 기능과 더불어)을 매우 성숙하고 효율적으로 구현한다. 도커 네트워킹은 커널의 네트워킹 스택을 하위 레벨의 기본 요소로 사용하여 상위 레벨의 네트워크 드라이버를 만든다. 간단히 말해서, 도커 네트워킹은 리눅스 네트워킹이다.

기존에 구현된 리눅스 커널 기능은 고성능과 견고성을 보장한다. 가장 중요한 점은 여러 배포판과 버전에 걸쳐 이식성을 제공하므로 애플리케이션의 이식성이 향샹된다는 것이다.

도커가 네이티브 CNM 네트워크 드라이버를 구현하는데 사용하는 몇 가지 리눅스 네트워킹 빌딩 블록이 있다. 이 목록에는 리눅스 브리지, 네트워크 네임스페이스, veth pair 및 iptable이 포함된다. 네트워크 드라이버로 구현되는 이러한 도구의 조합은 복잡한 네트워크 정책을 위한 전달 규칙, 네트워크 분할 및 관리 도구를 제공한다.

## 리눅스 브리지

리눅스 브리지는 리눅스 커널 내부의 물리적 스위치를 가상으로 구현한 Layer 2 디바이스이다. 트래픽을 검사하여 동적으로 학습되는 MAC 주소를 기반으로 트래픽을 전달한다. 리눅스 브리지는 많은 도커 네트워크 드라이버에서 광범위하게 사용된다. 리눅스 브리지는 리눅스 브리지의 상위 레벨 구현인 브리지 도커 네트워크 드라이버와 혼동되어서는 안된다.

## 네트워크 네임스페이스

리눅스 네트워크 네임스페이스는 커널에 격리된 네트워크 스택으로 자체 인터페이스, 라우트 및 방화벽 규칙을 지니고 있다. 컨테이너와 리눅스의 보안적인 특징으로, 컨테이너를 분리하는데 사용된다. 네트워킹 용어에서는 호스트 내부의 네트워크 제어와 데이터 플레인을 격리하는 VRF와 유사하다. 네트워크 네임 스페이스는 도커 네트워크를 통해 구성된 경우가 아니면 동일한 호스트의 두 컨테이너가 서로 통신하거나 호스트 자체와 통신할 수 없음을 보장한다. 일반적으로 CNM 네트워크 드라이버는 각 컨테이너에 대해 별도의 네임스페이스를 구현한다. 그러나 컨테이너는 동일한 네트워크 네임스페이스를 공유할 수도 있고 호스트의 네트워크 네임스페이스의 일부가 될 수도 있다. 호스트 네트워크 네임스페이스 컨테이너는 호스트 인터페이스와 호스트 라우팅 테이블이다. 이 네트워크 네임스페이스를 글로벌 네트워크 네임스페이스라고 한다.

## 가상 이더넷 디바이스

**가상 이더넷 디바이스 (virtual ethernet device)** 또는 **veth**는 두 네트워크 네임스페이스 사이의 연결선으로 동작하는 리눅스 네트워킹 인터페이스이다. veth는 각 네임스페이스에 단일 인터페이스가 있는 전이중 링크(full duplex link)이다. 한 인터페이스의 트래픽은 다른 인터페이스로 전달된다. 도커 네트워크를 만들 때 도커 네트워크 드라이버는 veth를 사용하여 네임스페이스간에 명시적인 연결을 제공한다. 컨테이너가 도커 네트워크에 연결되면 veth의 한쪽 끝은 컨테이너 내부에 배치되며 (일반적으로 `ethX` 인터페이스로 표시됨), 다른 쪽은 도커 네트워크에 연결된다.

## iptables

`iptables`는 네이티브 패킷 필터링 시스템으로 버전 2.4 이후 리눅스 커널의 일부였다. 이는 패킷 마킹, 매스쿼레이딩(masquerading, 위장) 및 드라핑(dropping, 삭제)에 대한 룰체인을 제공하는 기능이 풍부한 L3/L4 방화벽이다. 네이티브 도커 네트워크 드라이버는 iptables를 광범위하게 활용하여 네트워크 트래픽을 세분화하고, 호스트 포트 매핑을 제공하며, 로드밸런싱 결정을 위해 트래픽을 마킹한다. 

# 도커 네트워크 컨트롤 플레인

도커 분산형 네트워크 제어 플레인은 컨트롤 플레인 데이터를 전파하는 것 외에 Swarm 스코프 도커 네트워크의 상태를 관리한다. 도커 Swarm 클러스터의 빌트인 기능으로 외부 KV 저장소와 같은 추가 구성 요소가 필요하지 않다. 컨트롤 플레인에서는 SWIM에 기반한 가십 프로토콜을 사용하여 도커 컨테이너 클러스터에 네트워크 상태 정보와 토폴로지를 전파한다. 가십 프로토콜은 대규모 클러스터에서 메시지 크기, 오류 감지 시간 및 컨버전스 시간을 일정하게 유지하면서 클러스터 내에서 결과적 일관성(eventual consistency)을 달성하는 데 매우 효율적이다. 이는 네트워크가 느린 컨버전스 또는 잘못된 긍정 노드 오류와 같은 확장 문제를 일으키지 않고 네트워크가 여러 노도로 확장될 수 있다.

컨트롤 플레인은 암호화된 채널을 통해 기밀성, 무결성 및 인증을 제공하며 매우 안전하다. 또한 네트워크 마다 범위가 지정되어 있어 주어진 호스트에서 수신하는 업데이트를 크게 감소시킨다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fcontrolplane.png)

대규모 네트워크 전반에 걸쳐 빠른 컨버전스를 달성하기 위해서 함께 동작하는 몇 가지 컴포넌트로 구성되어 있다. 컨트롤 플레인의 분산 특성은 클러스터 컨트롤러 장애가 네트워크 성능에 영향을 미치지 않도록 보장한다.

도커 네트워크 컨트롤 플레인의 컴포넌트는 다음과 같다.

- **메세지 확산 (Message Dissemination)**은 각 교환의 정보를 보다 큰 노드 그룹으로 확장하는 P2P 방식으로 노드를 업데이트한다. 피어 그룹의 고정된 간격과 크기는 클러스터의 크기가 확장되는 경우에도 네트워크 사용량을 일정하도록 한다. 피어 간의 기하급수적인 정보 전파를 컨버전스가 모든 클러스터 크기에 대해 신속하고 제한적이도록 보장한다.
- **장애 탐지 (Failure Detection)**는 네트워크 정체와 잘못된 긍정 노드 장애를 유발하는 특정 경로를 배제시키기 위해 직접 및 간접 hello 메시지를 사용한다.
- **완전한 상태 동기화 (Full State Syncs)**는 일관성을 더 빨리 달성하고 네트워크 파티션을 해결하기 위해 주기적으로 발생한다.
- **토폴로지 인식 (Topology Aware)** 알고리즘은 자신과 다른 피어 간의 상대적인 대기 시간을 이해한다. 이것은 컨버전스를 보다 빠르고 효율적으로 만드른 피어 그룹을 최적화하는 데 사용된다.
- **Control Plane Encryption (Control Plane Encryption)**은 네트워크 보안을 손상시킬 수 있는 중간자 공격(man in the middle) 및 기타 공격에서 보호한다.

> 도커 네트워크 컨트롤 플레인은 [Swarm](https://docs.docker.com/engine/swarm/)의 구성 요소이며 Swarm 클러스터가 운영되어야 한다.

# 도커 호스트 네트워크 드라이버

`host` 네트워크 드라이버는 도커 없이 리눅스가 사용하는 것과 동일한 네트워킹 구성이기 때문에 도커가 처음인 사람들에게 가장 친숙한다. `-net=host`는 효과적으로 도커 네트워킹을 해제하고, 컨테이너는 호스트 운영체제의 호스트(또는 디폴트) 네트워킹 스택을 사용한다.

일반적으로 다른 네트워킹 드라이버의 경우, 각 컨테이너는 자체 네트워크 네임스페이스(또는 샌드박스)에 배치되어 서로 완전한 네트워크 격리를 제공한다. `host` 드라이버 컨테이너는 모두 동일한 호스트 네트워크 네임스페이스에 있으며 호스트의 네트워크 인터페이스와 IP 스택을 사용한다. 호스트 네트워크의 모든 컨테이너는 호스트 인터페이스에서 서로 통신할 수 있다. 네트워킹 관점에서 이는 컨테이너 없이 호스트에서 실행되는 여러 프로세스와 동일하다. 이들이 동일한 호스트 인터페이스를 사용하고 있기 때문에, 어떤 두 컨테이너도 똑같은 TCP 포트에 바인딩할 수 없다. 같은 호스트에서 여러 컨테이너가 스케줄링되는 경우 포트 경합이 발생할 수 있다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fhost-driver.png)

```sh
#Create containers on the host network
$ docker run -itd --net host --name C1 alpine sh
$ docker run -itd --net host --name nginx

#Show host eth0
$ ip add | grep eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
	inet 172.31.21.213/20 brd 172.31.31.255 scope global eth0

#Show eth0 from C1
$ docker run -it --net host --name C1 alpine ip add | grep eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP qlen 1000
	inet 172.31.21.213/20 brd 172.31.31.255 scope global eth0

#Contact the nginx container through localhost on C1
$ curl localhost
!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```


이 예에서, 컨테이너가 `host` 네트워크를 사용할 때 호스트, `C1` 및 `nginx`는 모두 `eth0`에 대해 동일한 인터페이스를 공유한다. 이는 `host`를 멀티 테넌트(Multi-tenant) 또는 보안 수준이 높은 애플리케이션에 적합하지 않게 만든다. `host` 컨테이너는 호스트의 다른 모든 컨테이너에 네트워크 액세스 권한을 가진다. `C1`에서 `curl nginx`를 실행할 때 예제와 같이 `localhost`를 사용하여 컨테이너 간에 통신이 가능하다.

`host` 드라이버를 사용하는 경우, 도커는 포트 매핑 또는 라우팅 규칙과 같은 컨테이너 네트워킹 스택의 어떤 부분도 관리하지 않는다. 이것은 `-p`와 `--icc`와 같은 일반적인 네트워킹 플래그가 호스트 드라이버에 아무런 의미가 없다는 것을 의미한다. 이들은 무시된다. 이는 `host` 네트워킹을 네트워킹 드라이버 중에서 가장 간단하게 그리고 가장 낮은 지연 시간(latency)을 갖게 한다. 트래픽 경로는 컨테이너 프로세스에서 호스트 인터페이스로 직접 이동하며, 컨테이너화되지 않은 프로세스와 동등한 베어메탈 성능을 제공한다.

전체 호스트 액세스와 자동화된 정책 관리가 없을 경우 `host` 드라이버가 일반 네트워크 드라이버로 적합하기 어려울 수 있다. 그러나, `host` 초고성능 애플리케이션 또는 애플리케이션 문제 해결(troubleshooting)과 같은 유즈케이스에 적용할 수 있는 몇 가지 흥미로운 속성을 가지고 있다.

# 도커 브리지 네트워크 드라이버

이 섹션에서는 기본 도커 브리지 네트워크와 사용자 정의 브리지 네트워크를 설명한다.

## 기본 도커 브리지 네트워크

도커 엔진을 실행하는 호스트에는 기본적으로 `bridge` 라는 로컬 도커 네트워크가 있다. 이 네트워크는 `docker0` 이라 불리는 리눅스 브리지를 인스턴스화하는 `bridge` 네트워크 드라이버를 사용하여 생성된다. 혼란스러울 수 있다.

- `bridge`는 도커 네트워크의 이름이다.
- `brdige`는 이 네트워크가 생성되는 네트워크 드라이버 또는 템플릿이다.
- `docker0`은 이 네트워크를 구현하는데 사용되는 커널 빌딩 블록인 리눅스 브리지의 이름이다.


독립 실행형 도커 호스트에서, `bridge`는 다른 네트워크가 지정되지 않은 경우 컨테이너가 연결되는 기본 네트워크다. 다음 예에서 컨테이너는 네트워크 매개 변수 없이 생성된다. 도커 엔진은 기본적으로 이를 브리지 네트워크에 연결한다. 컨테이너 내부 `eth0`을 주목하라. 이는 `bridge` 드라이버에 의해 생성되고 도커 네이티브 IPAM 드라이버에 의해 주소가 지정된다.

```sh
#Create a busybox container named "c1" and show its IP addresses
host $ docker run -it --name c1 busybox sh
c1 # ip address
4: eth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
	link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
	inet 172.17.0.2/16 scope global eth0
...
```

> 컨테이너 인터페이스의 MAC 주소는 동적으로 생성되고 충돌을 피하기 위해 IP 주소를 포함한다. 여기서 `ac:11:00:02`는 172.17.0.2에 해당한다.

호스트의 `brctl` 툴은 호스트 네트워크 네임 스페이스에 존재하는 리눅스 브릿지를 보여준다. `docker0`이라 부르는 단일 브릿지를 보여주고 있다. `docker0`에는 하나의 인터페이스 `vetha3788c4`가 있는데, 이 인터페이스는 브리지에서 컨테이너 `c1` 내부의 `eth0` 인터페이스에 대한 연결을 제공한다.

```sh
host $ brctl show
bridge name      bridge id            STP enabled    interfaces
docker0          8000.0242504b5200    no             vethb64e8b8
```
	
컨테이너 `c1` 내부의 컨테이너 라우팅 테이블은 컨테이너의 `eth0`으로 트래픽을 전달하고, 따라서 `docker0` 브리지로 이동한다.

```sh
c1# ip route
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0  src 172.17.0.2
```
	
컨테이너는 연결되는 네트워크 수에 따라 0개에서 다수의 인터페이스를 가질 수 있다. 각 도커 네트워크는 컨테이너 마다 하나의 인터페이스만 가질 수 있다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fbridge-driver.png)

호스트 라우팅 테이블이 보여주는 것처럼, 글로벌 네트워크 네임스페이스의 IP 인터페이스는 이제 `docker0`을 포함한다. 호스트 라우팅 테이블은 외부 네트워크에서 `doker0`과 `eth0` 사이의 연결시켜 컨테이너 내부에서 외부 네트워크로의 경로를 완성한다.

```sh
host $ ip route
default via 172.31.16.1 dev eth0
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.42.1
172.31.16.0/20 dev eth0  proto kernel  scope link  src 172.31.16.102
```

## 사용자 정의 도커 브리지 네트워크

기본 네트워크 외에도, 사용자들은 어떤 네트워크 드라이버 유형의 **사용자 정의 네트워크 (user-defined-network)** 라고 불리는 그들 자신의 네트워크를 만들 수 있다. 사용자 정의 `bridge` 네트워크의 경우 호스트에 새 리눅스 브리지를 설정한다. 기본 `bridge` 네트워크와 달리, 사용자 정의 네트워크는 수동 IP 주소와 서브넷 할당을 지원한다. 할당이 지정되지 않은 경우에는, 도커의 기본 IPAM 드라이버가 프라이빗 IP 공간에서 사용할 수 있는 다음 서브넷을 할당한다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fbridge2.png)

사용자 정의 `bridge` 네트워크 아래에는 2개의 컨테이너가 연결되어 있따. 서브넷이 지정되고 네트워크 이름은 `my_bridge`로 지정된다. 하나의 컨테이너에는 IP 매개변수가 제공되지 않으므로, IPAM 드라이버는 서브넷에 사용 가능한 다음 IP를 할당한다. 다른 컨테이너에는 IP가 지정되어 있다.

```sh
$ docker network create -d bridge --subnet 10.0.0.0/24 my_bridge
$ docker run -itd --name c2 --net my_bridge busybox sh
$ docker run -itd --name c3 --net my_bridge --ip 10.0.0.254 busybox sh
```

`brctl`은 이제 호스트에 두번째 리눅스 `bridge`를 나타낸다. 리눅스 브리지의 이름인 `br-4bcc22f5e5b9`는 `my_bridge` 네트워크의 네트워크 ID와 일치한다. `my_bridge`는 또한 컨테이너 `c2`와 `c3`에 연결된 2개의 `veth` 인터페이스가 있다.

```sh
$ brctl show
bridge name      bridge id            STP enabled    interfaces
br-b5db4578d8c9  8000.02428d936bb1    no             vethc9b3282
														vethf3ba8b5
docker0          8000.0242504b5200    no             vethb64e8b8

$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
b5db4578d8c9        my_bridge           bridge              local
e1cac9da3116        bridge              bridge              local
...
```

글로벌 네트워크 네임스페이스 인터페이스 목록은 도커 엔진에 의해 인스턴스화된 리눅스 네트워킹 서킷을 보여준다. 각 `veth` 및 리눅스 브리지 인터페이스는 리눅스 브리지 중 하나와 컨테이너 네트워크 네임스페이스 사이의 링크로 나타난다.

```sh
$ ip link

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
5: vethb64e8b8@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
6: br-b5db4578d8c9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
8: vethc9b3282@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
10: vethf3ba8b5@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
...
```

## 독립 실행형 컨테이너의 외부 접근

기본적으로 동일한 도커 네트워크의 모든 컨테이너 (멀티 호스트 swarm 스코프 또는 local 스코프)는 모든 포트에서 서로 연결된다. 서로 다른 도커 네트워크와 외부 도커에서 발생하는 컨테이너 ingress 트래픽 간의 통신은 방화벽으로 보호된다. 이것은 컨테이너 애플리케이션을 외부 세계 및 상호로부터 보호하는 기본적인 보안 측면이다. 이는 [네트워크 보안](https://success.docker.com/api/asset/.%2Frefarch%2Fnetworking%2F#security)에 보다 자세히 설명되어 있다.

대부분 유형의 도커 네트워크(`brdige`와 `overlay` 포함)의 경우 애플리케이션을 위한 외부 수신 액세스를 명시적으로 부여해야 한다. 이는 내부 포트 매핑을 통해 이루어진다. 도커는 내부 컨테이너 인터페이스에 호스트 인터페이스에 노출된 포트를 게시한다. 다음 다이어그램은 컨테이너 C2로의 ingress(수신, 하향 화살표) 및 engress(송신, 상단 화살표) 트래픽을 나타낸다. 아웃바운드(수신) 컨테이너 트래픽은 기본적으로 허용된다. 컨테이너에 의해 개시된 송신 연결은 임시 포트(일반적으로 32768 - 60999 범위)에 마스커레이딩되거나 SNAT된다. 이 연결에서 반송 트래픽(return traffic)이 허용되므로 컨테이너는 임시 포트에서 호스트의 가장 라우팅 가능한 IP 주소를 사용한다.

수신 액세스(ingress access)는 명시적인 포트 퍼블리싱을 통해 제공된다. 포트 퍼블리싱은 도커 엔진에 의해 수행되며 UCP 또는 엔진 CLI를 통해 제어할 수 있다. 서비스나 컨테이너를 노출하도록 특정 또는 무작위로 선택된 포트를 구성할 수 있다. 포트는 특정(또는 모든) 호스트 인터페이스에서 수신 대기하도록 설정할 수 있으며, 모든 트래픽은 이 포트에서 포트 및 컨테이너 내부의 인터페이스로 매핑된다.

```sh
$ docker run -d --name C2 --net my_bridge -p 5000:80 nginx
```

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fnat.png)

외부 액세스는 도커 CLI 또는 UCP에서 `--publish/-p`를 사용하여 구성되며, 위의 명령을 실행한 후 다이어그램은 컨테이너 `C2`가 `my_bridge` 네트워크에 연결되고 IP 주소가 `10.0.0.2`인 것으로 표시된다. 컨테이너는 `192.168.0.2`의 호스트 인터페이스의 포트 `5000`에서 외부 세계에 서비스를 알린다. 이 인터페이스:포트로 전송되는 모든 트래픽은 컨테이너 인터페이스의 `10.0.2:80`에 퍼블리싱된 포트다.

컨테이너에 의해 시작된 아웃바운드 트래픽은 호스트 인터페이스 `192.168.0.2`의 사용 후 삭제 포트 `32768`에서 소싱되도록 마스커레이딩된다. 반송 트래픽(return traffic)은 목적지와 동일한 IP 주소와 포트를 사용하며 내부적으로 다시 컨테이너 주소:포트 `10.0.2:33920`로 마스커레이딩된다. 포트 게시를 사용할 때, 네트워크의 외부 트래픽은 항상 호스트 IP와 노출된 포트를 사용하며 컨테이너 IP와 내부 포트는 사용하지 않는다.

도커 엔진의 클러스터에 컨테이너와 서비스를 노출하는 방법에 대한 자세한 내용은 [Swarm 서비스 외부 액세스](https://success.docker.com/api/asset/.%2Frefarch%2Fnetworking%2F#swarm-external)를 참조하라.

# 오버레이 드라이버 네트워크 아키텍처

네이티브 도커 `overlay` 네트워크 드라이버는 멀티 호스트 네트워킹의 여러가지 문제를 근본적으로 단순화한다. `overlay` 드라이버를 사용하면, 멀티 호스트 네트워크가 외부 프로비저닝이나 구성요소 없이 도커 내부의 일급 시민이다. `overlay`는 swarm 분산 컨트롤 플레인을 사용하여 대규모 클러스터에 걸쳐 중앙 집중식 관리, 안정성 및 보안을 제공한다.

## VXLAN 데이터 플레인

`overlay` 드라이버는 기본 물리적 네트워크(언더레이)와 컨테이너 네트워크를 분리하는 업계 표준 VXLAN 데이터 플레인을 활용한다. 도커 오버레이 네트워크는 컨테이너 트래픽을 VXLAN 헤더에 컨테이너 트래픽을 캡슐화하여 물리적 Layer 2 또는 Layer 3 네트워크를 통과할 수 있도록 한다. 오버레이를 사용하면 기본 물리적 토폴로지에 상관없이 네트워크 분할을 동적으로 제어하기 쉽다. 표준 IETF VXLAN 헤더를 사용해서 네트워크 트래픽을 검사하고 분석하기 위한 표준 툴링을 꾀할 수 있다.

> VXLAN은 버전 3.7부터 리눅스 커널의 일부였으며, 도커는 커널의 기본 VXLAN 기능을 사용하여 오버레이 네트워크를 생성한다. 도커 오버레이 데이터패스는 전적으로 커널 공간에 있다. 따라서 컨텍스트 스위치가 줄어들고 CPU 오버헤드가 감소하며, 대기 시간이 짧고, 애플리케이션과 물리적 NIC 간의 직접적인 트래픽 경로가 생긴다.

IETF VXLAN([RFC 7348](https://datatracker.ietf.org/doc/rfc7348/))은 Layer 3 네트워크에서 Layer 2 세그먼트를 오버레이하는 데이터 계층 캡슐화 형식이다. VXLAN은 표준 IP 네트워크에서 사용하도록 설계되었으며 공유된 물리적 네트워크 인프라에서 대규모 멀티 테넌트(Multi-tenant) 설계를 지원할 수 있다. 기존 사내 및 클라우드 기반 네트워크는 VXLAN을 투명하게 지원할 수 있다.

VXLAN은 언더레이 IP/UDP 헤더 내에 컨테이너 Layer 2 프레임을 배치하는 MAC-in-UDP 캡슐화로 정의된다. 언더레이 IP/UDP 헤더는 언더레이 네트워크의 호스트 간의 전송을 제공한다. 오버레이는 지정된 오버레이 네트워크에 참여하는 각 호스트 간에 점 대 다중점 연결로 존재하는 상태 비저장 VXLAN 터널이다. 오버레이는 언더레이 토폴로지와 독립적이기 때문에 애플리케이션이 더 쉽게 이동할 수 있다. 따라서, 네트워크 정책 및 연결은 온프레미스, 개발자 데스크탑, 또는 퍼블릭 클라우드에 관계없이 애플리케이션과 함께 전송될 수 있다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fpacketwalk.png)

이 다이어그램에서 오버레이 네트워크의 패킷 흐름이 표시된다. 다음은 `c1`이 공유 오버레이 네트워크에서 `c2` 패킷을 전송할 때 수행되는 단계이다.

- `c1`은 `c2`에 대한 DNS 조회를 한다. 두 컨테이너가 동일한 오버레이 네트워크에 있으므로 도커 엔진 로컬 DNS 서버는 `c2`를 오버레이 IP 주소 `10.0.0.3`으로 확인한다.
- 오버레이 네트워크는 L2 세그먼트이므로 `c1`은 `c2`의 MAC 주소로 지정된 L2 프레임을 생성한다.
- 프레임은 `overlay` 네트워크 드라이버에 의해 VXLAN 헤더로 캡슐화된다. 분산 오버레이 컨트롤 플레인은 각 VXLAN 터널 엔드포인트의 위치와 상태를 관리하므로 `c2`가 `192.168.0.3`의 물리적 주소에 있는 `host-B`에 있다는 것을 알고 있다. 이 주소는 언더레이 IP 헤더의 대상 주소가 된다.
- 일단 캡슐화되면 패킷이 전송된다. 물리적 네트워크는 VXLAN 패킷을 올바른 호스트로 라우팅하거나 브리징하는 역할을 한다.
- 패킷은 `host-B`의 `eth0` 인터페이스에 도착하며 `overlay` 네트워크 드라이버에 의해 캡슐 해제된다. `c1`의 원래 L2 프레임은 `c2`의 `eth0` 인터페이스와 수신 대기중인 애플리케이션까지 전달된다.

## 오버레이 드라이버 내부 아키텍처

도커 Swarm 컨트롤 플레인은 오버레이 네트워크에 대한 모든 프로비저닝을 자동화한다. VXLAN 구성 또는 리눅스 네트워킹 구성은 필요없다. 오버레이의 옵션 기능인 데이터 플레인 암호화도 네트워크가 생성될 때 오버레이 드라이버에 의해 자동으로 구성된다. 사용자 또는 네트워크 운영자는 네트워크를 정의하고 (`docker network create -d overlay ...`) 컨테이너를 해당 네트워크에 연결하기만 하면 된다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Foverlayarch.png)

오버레이 네트워크 생성 중에 도커 엔진은 각 호스트에 오버레이에 필요한 네트워크 인프라를 만든다. 리눅스 브리지는 연결된 VXLAN 인터페이스와 함께 오버레이별로 생성된다. 도커 엔진은 해당 네트워크에 연결된 컨테이너가 호스트에 예약되어 있을 때만 호스트의 오버레이 네트워크를 지능적으로 인스턴스화한다. 이렇게 하면 연결된 컨테이너가 없는 오버레이 네트워크의 무질서한 확산을 방지할 수 있다.

다음 예는 오버레이 네트워크를 생성하고 그 네트워크에 컨테이너를 연결한다. 도커 Swarm/UCP는 자동으로 오버레이 네트워크를 생성한다. 다음 예에서 Swarm 또는 UCP를 미리 설정해야 한다.

```shell
#Create an overlay named "ovnet" with the overlay driver
$ docker network create -d overlay --subnet 10.1.0.0/24 ovnet

#Create a service from an nginx image and connect it to the "ovnet" overlay network
$ docker service create --network ovnet nginx
```

오버레이 네트워크가 생성되면 호스트 내부에 여러 개의 인터페이스와 브리지가 생성되고, 이 컨테이너 내부에 두 개의 인터페이스도 생성된다는 점에 주목해야 한다.

```shell
# Peek into the container of this service to see its internal interfaces
conatiner$ ip address

#docker_gwbridge network
52: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
    link/ether 02:42:ac:14:00:06 brd ff:ff:ff:ff:ff:ff
    inet 172.20.0.6/16 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe14:6/64 scope link
       valid_lft forever preferred_lft forever

#overlay network interface
54: eth1@if53: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450
    link/ether 02:42:0a:01:00:03 brd ff:ff:ff:ff:ff:ff
    inet 10.1.0.3/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet 10.1.0.2/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:aff:fe01:3/64 scope link
       valid_lft forever preferred_lft forever
```

현재 호스트에 있는 두 개의 브리지에 해당하는 컨테이너 내부에 두 개의 인터페이스가 생성되었다. 오버레이 네트워크에서, 각 컨테이너에는 `overlay`와 `docker_gwbridge`를 각각 연결하는 최소한 두 개의 인터페이스가 있다.

| 브리지          | 목적    |
|---------------|---------| 
|overlay        |오버레이 네트워크로의 인그레스 및 엔그레스는  VXLAN이 캡슐화하고 (선택적으로) 동일한 오버레이 네트워크의 컨테이너 간에 전송되는 트래픽을 암호화함. 이 특정 오버레이에 참여하는 모든 호스트에 걸쳐 오버레이를 확장함. 호스트의 오버레이 서브넷당 하나씩 존재하며 지정된 특정 오버레이 네트워크 이름과 동일한 이름을 가지고 있음.|
|docker_gwbridge|클러스터에서 나가는 트래픽에 대한 엔그레스 브리지. 호스트당 하나의 `docker_gwbridgege` 만 존재. 이 브리지의 컨테이너 대 컨테이너 트래픽은 인그레스/엔그레스 트래픽 플로우만 허용함.|

> 도커 오버레이 드라이버는 도커 엔진 1.9 이후 존재해왔으며, 네트워크 상태를 관리하기 위해 외부 K/V 스토어가 필요했다. 도커 엔진 1.12는 더 이상 외부 저장소가 필요하지 않도록 컨트롤 플레인 스테이트를 도커 엔진에 통합했다.  또한 1.12는 암호화와 서비스 로드 밸런싱을 포함한 몇 가지 새로운 기능을 도입했다. 도입된 네트워킹 기능은 이러한 기능을 지원하는 도커 엔진 버전이 필요하며, 이러한 기능을 이전 버전의 도커 엔진과 함께 사용하는 것은 지원하지 않는다.

# 도커 서비스를 위한 외부 액세스

Swarm & UCP는 클러스터 포트 퍼블리싱 외부에서 서비스에 액세스한다. 서비스의 수신 및 송신은 중앙 집중식 게이트웨이에 의존하지 않고 특정 서비스 작업이 실행 중인 호스트에 인그레스/엔그레스가 분산된다. 서비스를 위한 포트 퍼블리싱에는 `host` 모드와 `ingress` 모드라는 두 가지 모드가 있다.

## 인그레스 모드 서비스 퍼블리싱

`ingress` 모드 포트 퍼블리싱은 [Swarm Routing Mesh](https://success.docker.com/api/asset/.%2Frefarch%2Fnetworking%2F#routingmesh)를 사용하여 서비스 작업 간에 로드 밸런싱을 적용한다. 인그레스 모드는 모든 UCP/Swarm 노드에 노출된 포트를 퍼블리싱한다. 퍼블리시된 포트로 들어오는 트래픽은 라우팅 메쉬에 의해 로드 밸런싱되고 라운드 로빈 로드 밸런싱을 통해 서비스의 정상적인 작업 중 하나로 이동된다. 특정 호스트가 서비스 작업을 실행하고 있지 않더라도 이 포트는 호스트에 퍼블리시되고 작업이 있는 호스트에 로드 밸런싱된다.

```shell
docker service create --replicas 2 --publish mode=ingress,target=80,published=8080 nginx
```

> `mode=ingress`는 서비스의 기본 모드다. 이 명령은 축약 버전 `-p 80:8080`으로도 가능하다. 포트 `8080`은 클러스터의 모든 호스트에 노출되며, 이 서비스에서는 두 컨테이너에 대한 로드 밸런성이 이루어진다. 

## 호스트 모드 서비스 퍼블리싱

`host` 모드 포트 퍼블리싱은 특정 서비스 작업이 실행 중인 호스트의 포트만 노출한다. 포트는 해당 호스트의 컨테이너에 직접 매핑된다. 포트 충돌을 방지하기 위해 각 호스트에서 특정 서비스의 단일 작업만 실행할 수 있다.

```shell
$ docker service create --replicas 2 --publish mode=host,target=80,published=8080 nginx
```

> `host` 모드는 `mode=host` 플래그를 필요하다. 이 두 컨테이너가 실행 중인 호스트에 포트 `8080`을 로컬로 퍼블리싱한다. 로드 밸런싱은 적용하지 않으므로, 해당 노드에 대한 트래픽은 로컬 컨테이너에만 전달된다. 복제본 수에 사용할 수 있는 포트가 충분하지 않은 경우, 포트 충돌이 발생할 수 있다.

## 인그레스 디자인

두 가지 퍼블리싱 모드 중 하나를 위한 좋은 유즈케이스가 많이 있다. `ingress` 모드는 여러 복제본이 있고, 해당 복제본 간에 로드 밸런싱이 필요한 서비스에 잘 작동한다. `host` 모드는 다른 툴에 의해 외부 서비스 검색이 이미 제공되어 있는 경우 잘 작동한다. 호스트 모드의 또 다른 좋은 유즈케이스는 호스트별로 한 번씩 존재하는 글로벌 컨테이너이다. 이러한 컨테이너는 해당 호스트에만 관련이 있고 그 서비스에 액세스할 때 로드 밸런싱을 원하지 않는 로컬 호스트에 대한 특정 정보(예: 모니터링 또는 로깅)를 노출할 수도 있다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fingress-vs-host.png)

# MACVLAN

`macvlan` 드라이버는 실제로 새로 구현된 진정한 네트워크 가상화 기술이다. 리눅스 구현은 매우 가볍다. 왜나하면 격리를 위해 리눅스 브리지를 사용하는 대신, 네트워크 격리와 물리적 네트워크 연결성을 강제하기 위해 리눅스 이더넷 인터페이스 또는 하위 인터페이스와 간단히 연결되기 때문이다.

MACVLAN은 여러가지 고유한 특징과 기능을 제공한다. 매우 단순하고 가벼운 아키텍처 덕택에 성능에 긍정적인 영향을 미친다. MACVLAN 드라이버는 포트 매핑 대신 컨테이너와 물리적 네트워크 간에 직접적으로 액세스한다. 또한 컨테이너는 물리적 네트워크의 서브넷에 있는 라우팅 가능한 IP 주소를 수신할 수 있다.

MACVLAN 유즈케이스는 다음을 포함할 수 있다.

- 대기 시간이 매우 짧은 애플리케이션
- 컨테이너가 외부 호스트 네트워크와 동일한 서브넷에 있고 IP를 외부 네트워크 호스트로 사용해야 하는 네트워크의 설계

`macvlan` 드라이버는 부모 인터페이스의 개념을 사용한다. 이 인터페이스는 `eth0`과 같은 물리적인 인터페이스, `eth0.10` (`VLAN 10`을 나타내는 `.10`)과 같은 802.1q VLAN 태깅을 위한 하위 인터페이스, 또는 두 이더넷 인터페이스를 단일 논리적 인터페이스에 결합하는 본드 호스트 어댑터와 같은 물리적 인터페이스일 수도 있다.

MACVLAN 네트워크를 구성할 때, 게이트웨이 주소가 필요하다. 게이트웨이는 네트워크 인프라에서 제공하는 호스트 외부에 있어야 한다. MACVLAN 네트워크는 동일한 네트워크에 있는 컨테이너 간의 액세스를 허용한다. 동일한 호스트의 다른 MACVLAN 네트워크 간 액세스는 호스트 외부로 라우팅하지 않으면 불가능하다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fmacvlanarch.png)

이 예제는 호스트의 `eth0`에 MACVLAN 네트워크를 바인딩한다. 또한 두 개의 컨테이너를 `mvnet` MACVLAN 네트워크에 연결하고 그들 사이에서 핑(ping)할 수 있다는 것을 보여준다. 각 컨테이너는 물리적 네트워크 서브넷 `192.168.0.0/24`에 주소가 있고, 기본 게이트웨이는 물리적 네트워크의 인터페이스이다.

```shell
#Creation of MACVLAN network "mvnet" bound to eth0 on the host
$ docker network create -d macvlan --subnet 192.168.0.0/24 --gateway 192.168.0.1 -o parent=eth0 mvnet

#Creation of containers on the "mvnet" network
$ docker run -itd --name c1 --net mvnet --ip 192.168.0.3 busybox sh
$ docker run -it --name c2 --net mvnet --ip 192.168.0.4 busybox sh
/ # ping 192.168.0.3
PING 127.0.0.1 (127.0.0.1): 56 data bytes
64 bytes from 127.0.0.1: icmp_seq=0 ttl=64 time=0.052 ms
```
이 다이어그램에서 볼 수 있듯이, `c1`과 `c2`는 호스트의 `eth0`에 연결된 `macvlan` 이라는 MACVLAN 네트워크를 통해 연결된다.

## MACVLAN을 사용한 VLAN 트렁킹

리눅스 호스트에 802.1q를 트렁킹하는 것은 운영에 있어 많은 이들에게 악명높을 정도로 고통스럽다. 재부팅에도 지속되려면 설정 파일을 변경해야 한다. 브리지가 관련되어 있으면, 물리적 NIC를 브리지로 옮기고 그 브리지가 IP 주소를 얻어와야 한다. `macvlan` 드라이버는 생성, 파괴 및 호스트 재부팅을 통해 MACVLAN 네트워크의 하위 인터페이스와 기타 컴포넌트를 완벽하게 관리한다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Ftrunk-macvlan.png)

`macvlan` 드라이버가 하위 인터페이스로 인스턴스화되면 호스트와 L2의 세그먼트 컨테이너에 대한 VLAN 트렁킹할 수 있다. `macvlan` 드라이버는 자동으로 하위 인터페이스를 만들고, 이를 컨테이너 인터페이스에 연결한다. 그 결과, 각 컨테이너는 다른 VLAN에 있으며 트래픽이 물리적 네트워크에서 라우팅되지 않는 한, 컨테이너 간의 통신은 불가능하다.

```shell
#Creation of  macvlan10 network in VLAN 10
$ docker network create -d macvlan --subnet 192.168.10.0/24 --gateway 192.168.10.1 -o parent=eth0.10 macvlan10

#Creation of  macvlan20 network in VLAN 20
$ docker network create -d macvlan --subnet 192.168.20.0/24 --gateway 192.168.20.1 -o parent=eth0.20 macvlan20

#Creation of containers on separate MACVLAN networks
$ docker run -itd --name c1--net macvlan10 --ip 192.168.10.2 busybox sh
$ docker run -it --name c2--net macvlan20 --ip 192.168.20.2 busybox sh
```

앞선 구성에서 하위 인터페이스를 상위 인터페이스로 사용하도록 구성된 `macvlan` 드라이버를 사용하여 두 개의 개별 네트워크를 생성했다. `macvlan` 드라이버는 하위 인터페이스를 생성하고 이를 호스트의 `eth0`과 컨테이너 인터페이스 간에 연결한다. 호스트 인터페이스와 업스트림 스위치는 `switchport mode trunk`로 설정해 주어야 VLAN이 태그되어 인터페이스를 통과할 수 있다. 하나 또는 그 이상의 컨테이너를 지정된 MACVLAN 네트워크에 연결해서 L2를 통해 세그먼트화된 복잡한 네트워크 정책을 만들 수 있다.

> 여러 개의 MAC 주소가 단일 호스트 인터페이스 뒤에 있으므로, MAC 필터링의 NIC 지원에 따라 인터페이스에서 비규칙 모드(promiscuous mode)를 활성화해야할 수도 있다.

# 무지정(None, 격리용) 네트워크 드라이버

호스트 네트워크 드라이버와 마찬가지로 `none` 네트워크 드라이버는 기본적으로 관리되지 않는 네트워킹 옵션이다. 도커 엔진은 컨테이너 내부에 인터페이스를 만들거나, 포트 매핑을 설정하거나, 연결을 위한 경로를 설치하지 않는다. `--net=none`을 사용하는 컨테이너는 다른 컨테이너와 호스트로부터 완전히 격리된다. 네트워킹 관리자 또는 외부 툴이 이 배관을 제공해야 한다. `none`을 사용하는 컨테이너는 루프백 인터페이스만 있고 다른 인터페이스는 없다.

호스트 드라이버와 달리, `none` 드라이버는 각 컨테이너에 대해 구분된 네임스페이스를 생성하지 않는다. 이는 컨테이너와 호스트 사이의 컨테이너 네트워크를 확실하게 격리시킨다.

> --net=none 또는 --net=host를 사용하는 컨테이너는 다른 Docker 네트워크에 연결할 수 없다.

# 물리적 네트워크 디자인 요구사항

도커 엔터프라즈 엔진 및 도커 네트워킹은 공통 데이터 센터 네트워크 인프라와 토폴로지에서 실행되도록 설계되었다. 중앙 집중식 컨트롤러와 무정지(fault-tolerance) 클러스터는 광범위한 네트워크 환경에서 호환성을 보장한다. 네트워킹 기능(네트워크 프로비저닝, MAC 학습, 오버레이 암호화)을 제공하는 구성요소는 도커 엔진, UCP 또는 리눅스 커널 자체의 일부이다. 네이티브 도커 네트워킹 드라이버를 실행하는 데 추가 구성 요소나 특수 네트워킹 기능이 필요하지 않다.

보다 구체적으로, 도커 네이티브 네트워크 드라이버는 다음에 대한 요구사항이 없다.

- 멀티캐스트
- 외부 키밸류 저장소
- 특정 라우팅 프로토콜
- 호스트 간의 Layer 2 종속성
- 스핀(spine) 및 리프(leaf), 전통적인 3티어 및 PoD 설계와 같은 특정 토폴로지. 이러한 토폴로지는 지원된다.

이는 모든 환경에서 애플리케이션 이식성을 향샹시키면서 애플리케이션에 필요한 성능 및 정책을 변함없이 획득하는 컨테이너 네트워킹 모델(CNM)과 일치한다.

# Swarm 네이티브 서비스 검색

도커는 단일 도커 엔진에서 실행되는 컨테이너와 도커 Swarm에서 실행되는 `task`에 대한 서비스 검색을 제공하기 위해 내장된 DNS를 사용한다. 도커 엔진에는 사용자 정의 브리지, 오버레이 및 MACVLAN 네트워크의 호스트에 있는 모든 컨테이너에 이름 분석(name resolution)을 하는 내부 DNS 서버가 있다. 각 도커 컨테이너(또는 Swarm 모드의 `task`)에는 DNS 리졸버가 있어 DNS 쿼리를 DNS 서버 역할을 하는 도커 엔진에 전달한다. 그런 다음 도커 엔진은 DNS 쿼리가 요청 컨테이너가 속한 네트워크의 컨테이너 또는 `service`에 속하는지 여부를 확인한다. 이 경우 도커 엔진은 키밸류 저장소에서 컨테이너, `task` 또는 `server`의 **이름**과 일치하는 IP 주소를 찾아 해당 IP 또는 서비스 가상 IP(VIP)를 다시 요청자에게 반환한다.

서비스 검색은 네트워크 스코프화되며, 동일한 네트워크에 있는 컨테이너 또는 태스크만 내장형 DNS 기능을 사용할 수 있음을 의미한다. 같은 네트워크에 있지 않은 컨테이너는 서로의 주소를 확인할 수 없다. 덧붙여, 네트워크의 DNS 항목이 있는 특정 네트워크 저장소에 컨테이너 또는 태스크가 있는 노드만 사용할 수 있다. 이는 보안과 성능을 증진시킨다.

대상 컨테이너 또는 `service`가 소스 컨테이너와 동일한 네트워크에 있지 않는 경우, 도커 엔진은 기본 DNS 서버를 구성하기 위해 도커 쿼리를 전달한다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2FDNS.png)

이 예에서는 `myservice`라고 하는 두 개의 컨테이너 서비스가 있다. 두 번째 서비스(`client`)는 동일한 네트워크에 존재한다. 그 `client`는 `docker.com`과 `myservice`에 대해 두 번의 curl 작업을 실행한다. 

다음은 발생하게 되는 액션이다.

- `docker.com` 및 `myservice`에 대한 DNS 쿼리가 `client`에 의해 시작된다.
- 컨테이너의 내장형 리졸버는 `127.0.11:53`의 DNS 쿼리를 가로채 도커 엔진의 DNS 서버로 보낸다.
- 내 `myservice`는 개별 태스크 IP 주소로 내부 로드 밸런싱되는 해당 서비스의 가상 IP(VIP)로 식별된다. IP 주소에 직접 연결되더라도 컨테이너 이름은 마찬가지로 리졸브된다.
- `docker.com`은 `mynet` 네트워크에 서비스 이름으로 존재하지 않으므로 요청은 설정된 기본 DNS 서버로 전달된다.

# 도커 네이티브 로드 밸런싱

도커 Swarm 클러스터에는 엔진에 바로 내장된 내부 및 외부 로드 밸런싱 기능이 있다. 내부 로드 밸런싱은 동일한 Swarm 또는 UCP 클러스터 내의 컨테이너 간에 로드 밸런싱을 제공한다. 외부 로드 밸런싱은 클러스터로 들어오는 수신 트래픽의 로드 밸런싱을 제공한다.

## UCP Internal Load Balancing

도커 서비스가 생성되면 내부 로드 밸런싱이 자동으로 인스턴스화된다. 도커 Swarm 클러스터에서 서비스가 생성되면 서비스 네트워크의 일부인 가상 IP(VIP)가 자동으로 할당된다. 서비스 이름을 확인하면 VIP가 반환된다. 그 VIP에 대한 트래픽은 오버레이 네트워크를 통해 해당 서비스의 모든 정상 업무로 자동 전송된다. 이 접근법은 단일 IP만 클라이언트로 반환되기 때문에 클라이언트 측 로드 밸런싱을 방지한다. 도커는 트래픽을 라우팅하고 균등하게 배분한다.

![](https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fipvs.png)

## UCP External L4 Load Balancing (Docker Routing Mesh)

## UCP External L7 Load Balancing (HTTP Routing Mesh)

# Docker Network Security and Encryption

## Network Segmentation and Data Plane Security

## Control Plane Security
## Data Plane Network Encryption
## Management Plane Security & RBAC with UCP

# IP Address Management

# Network Troubleshooting

# Network Deployment Models

## Bridge Driver on a Single Host
## Multi-Host Bridge Driver with External Service Discovery
## Multi-Host with Overlay Driver
### Overlay Benefits and Use Cases

## Tutorial App: MACVLAN Bridge Mode

### MACVLAN Benefits and Use Cases

# Conclusion







